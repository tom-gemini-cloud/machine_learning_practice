{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d7cf0f",
   "metadata": {},
   "source": [
    "# 3. Discriminant Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tom-gemini-cloud/machine_learning_practice/blob/main/ml_week_2/discriminant_analysis.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee782e52",
   "metadata": {},
   "source": [
    "## Discriminant Analysis\n",
    "\n",
    "Discriminant Analysis is a family of supervised classification methods that model each class's distribution and then use Bayes' rule to assign the most likely class.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Assume each data point is a feature vector:\n",
    "\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^d$ (a vector of $d$ real-valued features)\n",
    "\n",
    "Assume the features come from a **class-conditional density**:\n",
    "\n",
    "- $p(\\mathbf{x} \\mid y = k)$ (the probability density of seeing $\\mathbf{x}$ if the class is $k$)\n",
    "\n",
    "and each class has a **prior probability**:\n",
    "\n",
    "- $\\pi_k = P(y = k)$ (how common class $k$ is overall)\n",
    "\n",
    "> **Notation:** Lowercase $p$ denotes a probability _density_ (for continuous variables like $\\mathbf{x}$), while uppercase $P$ denotes a probability of a discrete event (like the class label $y = k$). Densities can exceed 1, but probabilities are always between 0 and 1.\n",
    "\n",
    "### Prediction rule\n",
    "\n",
    "Predict the class $\\hat{y}$ by choosing the class $k$ that maximises:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k \\left[ \\log p(\\mathbf{x} \\mid y = k) + \\log \\pi_k \\right]\n",
    "$$\n",
    "\n",
    "**In plain English:** For each possible class, compute a score by adding two things together:\n",
    "\n",
    "1. **How well the data fits that class** — $\\log p(\\mathbf{x} \\mid y = k)$ measures how likely we'd see this particular data point if it truly belonged to class $k$\n",
    "2. **How common that class is** — $\\log \\pi_k$ gives a \"bonus\" to classes that appear more frequently in the training data\n",
    "\n",
    "Then pick whichever class has the highest total score. The $\\arg\\max_k$ simply means \"find the value of $k$ that gives the maximum\".\n",
    "\n",
    "We use logarithms because they turn multiplication into addition (making computation easier) and don't change which class wins (since log is monotonically increasing).\n",
    "\n",
    "### Common modelling choice\n",
    "\n",
    "Most commonly, $p(\\mathbf{x} \\mid y = k)$ is modelled as a **multivariate Gaussian**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561fe8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA assumes that all classes share the **same covariance matrix** $\\Sigma$. Each class $k$ has its own mean $\\boldsymbol{\\mu}_k$, but the shape and orientation of the distribution is identical across classes.\n",
    "\n",
    "### The Model\n",
    "\n",
    "For each class $k$, the class-conditional density is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid y = k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n",
    "$$\n",
    "\n",
    "### The Discriminant Function\n",
    "\n",
    "Taking the log and dropping terms that don't depend on $k$, we get the **linear discriminant function**:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \\mathbf{x}^\\top \\Sigma^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\boldsymbol{\\mu}_k^\\top \\Sigma^{-1} \\boldsymbol{\\mu}_k + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "This is **linear in $\\mathbf{x}$** — hence the name \"Linear\" Discriminant Analysis.\n",
    "\n",
    "### Parameters to Estimate\n",
    "\n",
    "From training data, we estimate:\n",
    "\n",
    "1. **Class priors:** $\\hat{\\pi}_k = n_k / n$ (proportion of samples in class $k$)\n",
    "2. **Class means:** $\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i$\n",
    "3. **Shared covariance:** $\\hat{\\Sigma} = \\frac{1}{n - K} \\sum_{k=1}^{K} \\sum_{i: y_i = k} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^\\top$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sdmt9gk1qf8",
   "metadata": {},
   "outputs": [],
   "source": "# Step 0: Generate sample data\nimport numpy as np\n\nnp.random.seed(42)\n\n# Create two classes with different means but we'll assume same covariance\nn_samples_per_class = 100\nd = 2  # 2 features for easy visualisation\n\n# Class 0: centred at (0, 0)\nmean_0 = np.array([0, 0])\n# Class 1: centred at (2, 2)\nmean_1 = np.array([2, 2])\n\n# Shared covariance matrix\ncov = np.array([[1.0, 0.5],\n                [0.5, 1.0]])\n\nX_0 = np.random.multivariate_normal(mean_0, cov, n_samples_per_class)\nX_1 = np.random.multivariate_normal(mean_1, cov, n_samples_per_class)\n\nX = np.vstack([X_0, X_1])\ny = np.array([0] * n_samples_per_class + [1] * n_samples_per_class)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"Classes: {np.unique(y)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n0ial6f4yuk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Estimate class priors (π_k = n_k / n)\n",
    "classes = np.unique(y)\n",
    "n = len(y)\n",
    "\n",
    "priors = {}\n",
    "for k in classes:\n",
    "    n_k = np.sum(y == k)\n",
    "    priors[k] = n_k / n\n",
    "\n",
    "print(\"Class priors:\")\n",
    "for k, pi_k in priors.items():\n",
    "    print(f\"  π_{k} = {pi_k:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0s9wv05gu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Estimate class means (μ_k = mean of samples in class k)\n",
    "means = {}\n",
    "for k in classes:\n",
    "    X_k = X[y == k]  # Get all samples belonging to class k\n",
    "    means[k] = np.mean(X_k, axis=0)\n",
    "\n",
    "print(\"Class means:\")\n",
    "for k, mu_k in means.items():\n",
    "    print(f\"  μ_{k} = {mu_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shlp849mkl",
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Estimate shared covariance matrix (pooled within-class covariance)\n# Σ = (1 / (n - K)) * Σ_k Σ_i (x_i - μ_k)(x_i - μ_k)^T\n\nK = len(classes)\nd = X.shape[1]\n\n# Accumulate the sum of outer products\ncov_sum = np.zeros((d, d))\nfor k in classes:\n    X_k = X[y == k]\n    mu_k = means[k]\n    # Centre the data for this class\n    X_centred = X_k - mu_k\n    # Add contribution to covariance sum\n    cov_sum += X_centred.T @ X_centred\n\n# Divide by (n - K) for unbiased estimate\nshared_cov = cov_sum / (n - K)\n\nprint(\"Shared covariance matrix:\")\nprint(shared_cov)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfzrxg1qnhj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute discriminant function and predict\n",
    "# δ_k(x) = x^T Σ^{-1} μ_k - (1/2) μ_k^T Σ^{-1} μ_k + log(π_k)\n",
    "\n",
    "# Compute inverse of shared covariance\n",
    "cov_inv = np.linalg.inv(shared_cov)\n",
    "\n",
    "def lda_discriminant(x, mu_k, cov_inv, pi_k):\n",
    "    \"\"\"Compute LDA discriminant function for a single class.\"\"\"\n",
    "    term1 = x @ cov_inv @ mu_k\n",
    "    term2 = 0.5 * mu_k @ cov_inv @ mu_k\n",
    "    term3 = np.log(pi_k)\n",
    "    return term1 - term2 + term3\n",
    "\n",
    "def lda_predict(X, means, cov_inv, priors, classes):\n",
    "    \"\"\"Predict class labels for all samples.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    predictions = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        # Compute discriminant for each class\n",
    "        scores = {}\n",
    "        for k in classes:\n",
    "            scores[k] = lda_discriminant(x, means[k], cov_inv, priors[k])\n",
    "        # Predict class with highest score\n",
    "        predictions[i] = max(scores, key=scores.get)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lda_predict(X, means, cov_inv, priors, classes)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"LDA Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxrlvdw7ugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Verify against scikit-learn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "sklearn_lda = LinearDiscriminantAnalysis()\n",
    "sklearn_lda.fit(X, y)\n",
    "y_pred_sklearn = sklearn_lda.predict(X)\n",
    "\n",
    "sklearn_accuracy = np.mean(y_pred_sklearn == y)\n",
    "print(f\"scikit-learn LDA Accuracy: {sklearn_accuracy:.2%}\")\n",
    "print(f\"Predictions match: {np.all(y_pred == y_pred_sklearn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dvl2n71lwog",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "QDA relaxes the LDA assumption: each class $k$ can have its **own covariance matrix** $\\Sigma_k$. This allows for more flexible, curved decision boundaries.\n",
    "\n",
    "### The Model\n",
    "\n",
    "For each class $k$, the class-conditional density is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid y = k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n",
    "$$\n",
    "\n",
    "Note: now we have $\\Sigma_k$ instead of a shared $\\Sigma$.\n",
    "\n",
    "### The Discriminant Function\n",
    "\n",
    "Taking the log, we get the **quadratic discriminant function**:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "This is **quadratic in $\\mathbf{x}$** because of the $\\mathbf{x}^\\top \\Sigma_k^{-1} \\mathbf{x}$ term (which doesn't cancel out since each class has a different $\\Sigma_k$).\n",
    "\n",
    "### Parameters to Estimate\n",
    "\n",
    "From training data, we estimate:\n",
    "\n",
    "1. **Class priors:** $\\hat{\\pi}_k = n_k / n$\n",
    "2. **Class means:** $\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i$\n",
    "3. **Class-specific covariances:** $\\hat{\\Sigma}_k = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^\\top$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzr2v79ss7",
   "metadata": {},
   "outputs": [],
   "source": "# Step 0: Generate sample data with DIFFERENT covariances per class\nnp.random.seed(42)\n\nn_samples_per_class = 100\n\n# Class 0: centred at (0, 0) with one covariance\nmean_0 = np.array([0, 0])\ncov_0 = np.array([[1.0, 0.2],\n                  [0.2, 1.0]])\n\n# Class 1: centred at (2, 2) with a DIFFERENT covariance\nmean_1 = np.array([2, 2])\ncov_1 = np.array([[1.5, -0.8],\n                  [-0.8, 1.5]])\n\nX_0 = np.random.multivariate_normal(mean_0, cov_0, n_samples_per_class)\nX_1 = np.random.multivariate_normal(mean_1, cov_1, n_samples_per_class)\n\nX_qda = np.vstack([X_0, X_1])\ny_qda = np.array([0] * n_samples_per_class + [1] * n_samples_per_class)\n\nprint(f\"X shape: {X_qda.shape}\")\nprint(f\"Classes have DIFFERENT covariances - QDA should work better here\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19k3eizw8j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 1-2: Estimate class priors and means (same as LDA)\n",
    "classes_qda = np.unique(y_qda)\n",
    "n_qda = len(y_qda)\n",
    "\n",
    "priors_qda = {}\n",
    "means_qda = {}\n",
    "\n",
    "for k in classes_qda:\n",
    "    n_k = np.sum(y_qda == k)\n",
    "    priors_qda[k] = n_k / n_qda\n",
    "    means_qda[k] = np.mean(X_qda[y_qda == k], axis=0)\n",
    "\n",
    "print(\"Class priors:\")\n",
    "for k, pi_k in priors_qda.items():\n",
    "    print(f\"  π_{k} = {pi_k:.3f}\")\n",
    "\n",
    "print(\"\\nClass means:\")\n",
    "for k, mu_k in means_qda.items():\n",
    "    print(f\"  μ_{k} = {mu_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otdkvjfndfm",
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Estimate class-specific covariance matrices (KEY DIFFERENCE from LDA)\n# Σ_k = (1 / (n_k - 1)) * Σ_i (x_i - μ_k)(x_i - μ_k)^T  for samples in class k\n\ncovariances_qda = {}\n\nfor k in classes_qda:\n    X_k = X_qda[y_qda == k]\n    mu_k = means_qda[k]\n    n_k = len(X_k)\n    \n    # Centre the data\n    X_centred = X_k - mu_k\n    \n    # Compute covariance for this class (unbiased estimate)\n    covariances_qda[k] = (X_centred.T @ X_centred) / (n_k - 1)\n\nprint(\"Class-specific covariance matrices:\")\nfor k, cov_k in covariances_qda.items():\n    print(f\"\\nΣ_{k} =\")\n    print(cov_k)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bwgkw1qoxw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute quadratic discriminant function and predict\n",
    "# δ_k(x) = -0.5 * log|Σ_k| - 0.5 * (x - μ_k)^T Σ_k^{-1} (x - μ_k) + log(π_k)\n",
    "\n",
    "def qda_discriminant(x, mu_k, cov_k, cov_k_inv, pi_k):\n",
    "    \"\"\"Compute QDA discriminant function for a single class.\"\"\"\n",
    "    diff = x - mu_k\n",
    "    \n",
    "    term1 = -0.5 * np.log(np.linalg.det(cov_k))  # log determinant penalty\n",
    "    term2 = -0.5 * (diff @ cov_k_inv @ diff)      # Mahalanobis distance\n",
    "    term3 = np.log(pi_k)                          # prior bonus\n",
    "    \n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def qda_predict(X, means, covariances, priors, classes):\n",
    "    \"\"\"Predict class labels for all samples.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    predictions = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    # Precompute inverses\n",
    "    cov_invs = {k: np.linalg.inv(covariances[k]) for k in classes}\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        scores = {}\n",
    "        for k in classes:\n",
    "            scores[k] = qda_discriminant(\n",
    "                x, means[k], covariances[k], cov_invs[k], priors[k]\n",
    "            )\n",
    "        predictions[i] = max(scores, key=scores.get)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Make predictions\n",
    "y_pred_qda = qda_predict(X_qda, means_qda, covariances_qda, priors_qda, classes_qda)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_qda = np.mean(y_pred_qda == y_qda)\n",
    "print(f\"QDA Accuracy: {accuracy_qda:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5id9ev9k1o2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Verify against scikit-learn\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "sklearn_qda = QuadraticDiscriminantAnalysis()\n",
    "sklearn_qda.fit(X_qda, y_qda)\n",
    "y_pred_sklearn_qda = sklearn_qda.predict(X_qda)\n",
    "\n",
    "sklearn_accuracy_qda = np.mean(y_pred_sklearn_qda == y_qda)\n",
    "print(f\"scikit-learn QDA Accuracy: {sklearn_accuracy_qda:.2%}\")\n",
    "print(f\"Predictions match: {np.all(y_pred_qda == y_pred_sklearn_qda)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domkftn92ne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compare LDA vs QDA on this dataset\n",
    "# Since classes have different covariances, QDA should have an advantage\n",
    "\n",
    "sklearn_lda_compare = LinearDiscriminantAnalysis()\n",
    "sklearn_lda_compare.fit(X_qda, y_qda)\n",
    "lda_accuracy_compare = sklearn_lda_compare.score(X_qda, y_qda)\n",
    "\n",
    "print(\"Comparison on data with DIFFERENT class covariances:\")\n",
    "print(f\"  LDA Accuracy: {lda_accuracy_compare:.2%}\")\n",
    "print(f\"  QDA Accuracy: {sklearn_accuracy_qda:.2%}\")\n",
    "print(f\"\\nQDA performs better because it can model the different covariance structures.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}